# Whether training should be resumed from a previous checkpoint. Use a path saved by
# `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.
resume_from_checkpoint: "latest"
use_original_cond_image: False
use_fill_50k_dataset: False
overfit_on_single_image: False
probability_zero_out_input: 0.0
run_optuna: False
control_type_1: depth
control_type_2: hed
#zero_out_control_embeddings: True
#zero_out_control_net_text_embeddings: False
#zero_out_unet_text_embeddings: False
pretrained_model_name_or_path: "stabilityai/stable-diffusion-2-1-base"
dataset_size: 100000
# Batch size (per device) for the training dataloader.
train_batch_size: 1
# Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via
# `--resume_from_checkpoint`. In the case that the checkpoint is better than the final trained model,
# the checkpoint can also be used for inference. Using a checkpoint for inference requires separate loading of the
# original pipeline and the individual checkpointed model components.
# See https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint
# for step by step instructions.
checkpointing_steps: 10
num_train_epochs: 60
# Run validation every X steps. Validation consists of running the prompt
# `args.validation_prompt` multiple times: `args.num_validation_images`
# and logging the images.
validation_steps: 20
learning_rate: 5e-6
# Max number of checkpoints to store.
checkpoints_total_limit: 2
tokenizer_max_length: 77
# Number of updates steps to accumulate before performing a backward/update pass.
gradient_accumulation_steps: 32
# Whether to use gradient checkpointing to save memory at the expense of slower backward pass.
gradient_checkpointing: True
# Whether to use 8-bit Adam from bitsandbytes.
use_8bit_adam: True
# Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be
# float32 precision.
revision:
# Pretrained tokenizer name or path if not the same as model_name
tokenizer_name:
# The directory where the downloaded models and datasets will be stored.
cache_dir:
# A seed for reproducible training.
seed: 1
# The resolution for input images, all the images in the train/validation dataset will be resized to this resolution.
resolution: 512
# Total number of training steps to perform. If provided, overrides num_train_epochs.
max_train_steps:
# Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
scale_lr: False
# The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",
# "constant", "constant_with_warmup"]
lr_scheduler: "constant"
# Number of steps for the warmup in the lr scheduler.
lr_warmup_steps: 1000
# Number of hard resets of the lr in cosine_with_restarts scheduler.
lr_num_cycles: 1
# Power factor of the polynomial scheduler.
lr_power: 1.0
# Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
dataloader_num_workers: 0
# The beta1 parameter for the Adam optimizer.
adam_beta1: 0.5
# The beta2 parameter for the Adam optimizer.
adam_beta2: 0.999
# Weight decay to use.
adam_weight_decay: 0.01
# Epsilon value for the Adam optimizer.
adam_epsilon: 1e-08
# Max gradient norm.
max_grad_norm: 1.0
# Whether to push the model to the Hub.
push_to_hub: False
# The token to use to push to the Model Hub.
hub_token:
# The name of the repository to keep in sync with the local `output_dir`.
hub_model_id:
# [TensorBoard](https://www.tensorflow.org/tensorboard) log directory.
# Will default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.
logging_dir: "logs"
# Whether to allow TF32 on Ampere GPUs. Can be used to speed up training.
# For more information, see https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
allow_tf32: False
# The integration to report the results and logs to. Supported platforms are `"tensorboard"`
# (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.
report_to:
# Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=
# 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the
# flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.
# ["no", "fp16", "bf16"]
mixed_precision: "fp16"
enable_xformers_memory_efficient_attention: False
# Save more memory by using setting grads to None instead of zero.
# Be aware that this changes certain behaviors, so disable this argument if it causes any problems.
# More info: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html
set_grads_to_none: True
# For debugging purposes or quicker training, truncate the number of training examples to this value if set.
max_train_samples:
# Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).
proportion_empty_prompts: 0
# A set of prompts evaluated every `--validation_steps` and logged to `--report_to`.
# Provide either a matching number of `--validation_image`s, a single `--validation_image`
# to be used with all prompts, or a single prompt that will be used with all `--validation_image`s.
validation_prompt:
# A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`
# and logged to `--report_to`. Provide either a matching number of `--validation_prompt`s,
# a single `--validation_prompt` to be used with all `--validation_image`s, or a single
# `--validation_image` that will be used with all `--validation_prompt`s.
validation_image:
# Number of images to be generated for each `--validation_image`, `--validation_prompt` pair.
num_validation_images:
# The `project_name` argument passed to Accelerator.init_trackers for
# more information see https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator
tracker_project_name: "InterSem"